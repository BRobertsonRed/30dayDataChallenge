# Day 14 - Data Pipeline for ETL (Extract, Transform, Load)

## Project Overview
In this project, youâ€™ll build a data pipeline to automate the ETL process using SQL. The goal is to extract raw data, transform it for analysis, and load it into a data warehouse for reporting.

## Objectives
- Set up a data pipeline for automating the ETL process.
- Write SQL queries for data extraction, transformation, and loading.
- Automate data loads and schedule ETL jobs.

## Dataset
- **Name:** Sales Transactions Data
- **Source:** [Sales Dataset (Kaggle)](https://www.kaggle.com/datasets/einsteindata4u/sales-dataset)
- **Description:** Sales transactions data used to design and automate the ETL process.

## Steps to Complete
1. Import the dataset and design the ETL pipeline.
2. Write SQL queries to extract, clean, and transform the data.
3. Set up automated data loads using scheduling tools (e.g., cron jobs).
4. Load the transformed data into a data warehouse for analysis.
5. Document the ETL process and key learnings in a report.

## Checklist
- [ ] Download and explore the dataset.
- [ ] Write SQL queries for ETL tasks.
- [ ] Automate the data pipeline and schedule jobs.
- [ ] Load data into a data warehouse.
- [ ] Complete and document the project.

## Additional Resources
- [Guided Project: Building ETL Pipelines](https://www.coursera.org/learn/data-pipelines-etl)
- [ETL Process Guide](https://www.talend.com/resources/what-is-etl/)
